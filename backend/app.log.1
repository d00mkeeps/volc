2024-10-21 13:28:11,002 - app.services.llm_service - INFO - Stream created successfully
2024-10-21 13:28:11,005 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-21 13:28:11,866 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-10-21 13:28:11,866 - httpcore.http11 - DEBUG - response_closed.started
2024-10-21 13:28:11,866 - httpcore.http11 - DEBUG - response_closed.complete
2024-10-21 13:28:29,348 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:28:29,348 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:28:29,348 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:28:29,350 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:28:29,351 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:28:29,359 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:29,360 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:28:29,360 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:28:29,360 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:28:29,361 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:28:29,361 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:28:29,369 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:34,340 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:28:34,340 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:28:34,340 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:28:34,341 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:28:34,341 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:28:34,351 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:34,353 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:28:34,353 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:28:34,353 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:28:34,354 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:28:34,354 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:28:34,362 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:38,487 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:28:38,488 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:28:38,488 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:28:38,489 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:28:38,490 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:28:38,499 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:39,863 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:28:39,864 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:28:39,864 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:28:39,865 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:28:39,867 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:28:39,885 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:43,723 - app.services.llm_service - INFO - Processing message stream with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:28:43,724 - app.services.llm_service - DEBUG - Messages: [{'role': 'user', 'content': 'Message 1'}]
2024-10-21 13:28:43,728 - anthropic._base_client - DEBUG - Request options: {'method': 'post', 'url': '/v1/messages', 'timeout': 600, 'files': None, 'json_data': {'max_tokens': 250, 'messages': [{'role': 'user', 'content': 'Message 1'}], 'model': 'claude-3-5-sonnet-20240620', 'stream': True, 'system': 'Default system prompt here', 'temperature': 0.5}}
2024-10-21 13:28:43,730 - httpcore.connection - DEBUG - connect_tcp.started host='api.anthropic.com' port=443 local_address=None timeout=600 socket_options=None
2024-10-21 13:28:43,745 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b1e4a10>
2024-10-21 13:28:43,745 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x10b18e250> server_hostname='api.anthropic.com' timeout=600
2024-10-21 13:28:43,763 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10b1e4aa0>
2024-10-21 13:28:43,763 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-10-21 13:28:43,764 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-10-21 13:28:43,764 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-10-21 13:28:43,764 - httpcore.http11 - DEBUG - send_request_body.complete
2024-10-21 13:28:43,764 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-10-21 13:28:44,150 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 21 Oct 2024 12:28:44 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'no-cache'), (b'anthropic-ratelimit-requests-limit', b'50'), (b'anthropic-ratelimit-requests-remaining', b'49'), (b'anthropic-ratelimit-requests-reset', b'2024-10-21T12:29:28Z'), (b'anthropic-ratelimit-tokens-limit', b'40000'), (b'anthropic-ratelimit-tokens-remaining', b'40000'), (b'anthropic-ratelimit-tokens-reset', b'2024-10-21T12:28:44Z'), (b'request-id', b'req_017362jBp13MNMKrcijg8Bhq'), (b'via', b'1.1 google'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Robots-Tag', b'none'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8d613fc5cf033634-MAN')])
2024-10-21 13:28:44,151 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 200 OK"
2024-10-21 13:28:44,152 - anthropic._base_client - DEBUG - HTTP Request: POST https://api.anthropic.com/v1/messages "200 OK"
2024-10-21 13:28:44,152 - app.services.llm_service - INFO - Stream created successfully
2024-10-21 13:28:44,152 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-10-21 13:28:45,053 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-10-21 13:28:45,054 - httpcore.http11 - DEBUG - response_closed.started
2024-10-21 13:28:45,055 - httpcore.http11 - DEBUG - response_closed.complete
2024-10-21 13:30:42,876 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-21 13:30:42,878 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-21 13:30:42,878 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Default system prompt here'}
2024-10-21 13:30:42,880 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-21 13:30:42,882 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-21 13:30:42,898 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Default system prompt here'
2024-10-21 13:30:42,900 - app.services.llm_service - INFO - Creating LLMService with config: default
