2024-10-25 18:50:03,299 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 18:50:03,306 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 18:50:03,323 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 18:54:47,206 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 18:54:47,206 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 18:54:47,207 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 18:54:47,210 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 18:54:47,219 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 18:54:47,231 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 18:54:47,609 - httpcore.connection - DEBUG - close.started
2024-10-25 18:54:47,610 - httpcore.connection - DEBUG - close.started
2024-10-25 18:54:47,622 - httpcore.connection - DEBUG - close.complete
2024-10-25 18:54:47,624 - httpcore.connection - DEBUG - close.complete
2024-10-25 18:54:50,520 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 18:54:50,520 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 18:54:50,520 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 18:54:50,521 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 18:54:50,521 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 18:54:50,532 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 18:54:53,239 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 18:54:53,239 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 18:54:53,239 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 18:54:53,240 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 18:54:53,240 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 18:54:53,251 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 18:54:56,609 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 18:54:56,609 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 18:54:56,609 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 18:54:56,610 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 18:54:56,611 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 18:54:56,619 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 19:01:41,828 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 19:01:41,829 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 19:01:41,829 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 19:01:41,833 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 19:01:41,842 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 19:01:41,853 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 19:01:44,595 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 19:01:44,596 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 19:01:44,596 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 19:01:44,596 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 19:01:44,597 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 19:01:44,605 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 19:02:31,008 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 19:02:31,009 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 19:02:31,009 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 19:02:31,014 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 19:02:31,025 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 19:02:31,041 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 19:02:33,544 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-10-25 19:02:33,545 - app.core.llm_config - INFO - Getting LLM config for: default
2024-10-25 19:02:33,545 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-20240620', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-10-25 19:02:33,546 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-10-25 19:02:33,547 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-10-25 19:02:33,560 - app.services.llm_service - INFO - LLMService initialized with config: model='claude-3-5-sonnet-20240620' max_tokens=250 temperature=0.5 system_prompt='Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'
2024-10-25 19:04:48,679 - app.services.llm_service - INFO - Creating LLMService with config: default
