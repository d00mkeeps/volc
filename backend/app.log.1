2024-11-12 15:15:20,150 - anthropic._base_client - DEBUG - Request options: {'method': 'post', 'url': '/v1/messages', 'timeout': 600, 'files': None, 'json_data': {'max_tokens': 250, 'messages': [{'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': "I'm ready to help! What's your message?"}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': "I'm still waiting for your actual message. What would you like to discuss?"}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': "I can't help unless you share your actual message or question. What's on your mind?"}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'I need you to type out your actual message or question - just saying "Message" doesn\'t tell me what you need help with.'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'I can only help if you share your actual message or question, rather than just typing "Message". What would you like to discuss?'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'Please share what you actually want to discuss. Simply writing "Message" doesn\'t give me anything to respond to. What\'s your question?'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'You keep typing "Message" but I need your actual message or question to help you. What would you like to discuss?'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'I still need your actual message or question - just typing "Message" doesn\'t tell me what you need help with. Please share what\'s on your mind.'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'You\'re just repeating "Message" without sharing any actual content. Please tell me what you\'d like to discuss or ask.'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'I cannot help you if you only type "Message". Please share your actual question or topic you\'d like to discuss.'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'Please share your actual message or question - just writing "Message" over and over doesn\'t give me anything to respond to.'}, {'role': 'user', 'content': 'Message'}, {'role': 'assistant', 'content': 'Simply typing "Message" doesn\'t tell me what you need. Please share your actual question or topic you\'d like to discuss.'}, {'role': 'user', 'content': 'Message'}], 'model': 'claude-3-5-sonnet-latest', 'stream': True, 'system': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.', 'temperature': 0.5}}
2024-11-12 15:15:20,152 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-11-12 15:15:20,152 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-11-12 15:15:20,152 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-11-12 15:15:20,153 - httpcore.http11 - DEBUG - send_request_body.complete
2024-11-12 15:15:20,153 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-11-12 15:15:20,932 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Nov 2024 15:15:20 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'no-cache'), (b'anthropic-ratelimit-requests-limit', b'50'), (b'anthropic-ratelimit-requests-remaining', b'48'), (b'anthropic-ratelimit-requests-reset', b'2024-11-12T15:15:28Z'), (b'anthropic-ratelimit-tokens-limit', b'40000'), (b'anthropic-ratelimit-tokens-remaining', b'39000'), (b'anthropic-ratelimit-tokens-reset', b'2024-11-12T15:15:21Z'), (b'request-id', b'req_01T4a55y7XgFof5KavKTpERZ'), (b'via', b'1.1 google'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Robots-Tag', b'none'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e177a139e172207-MAN')])
2024-11-12 15:15:20,933 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 200 OK"
2024-11-12 15:15:20,933 - anthropic._base_client - DEBUG - HTTP Request: POST https://api.anthropic.com/v1/messages "200 OK"
2024-11-12 15:15:20,934 - app.services.llm_service - INFO - Stream created successfully
2024-11-12 15:15:20,934 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-11-12 15:15:21,166 - app.services.llm_service - ERROR - Error sending chunk: 
2024-11-12 15:15:21,166 - app.api.endpoints.llm - ERROR - Error in websocket: WebSocket is not connected. Need to call "accept" first.
2024-11-12 15:15:21,166 - app.api.endpoints.llm - ERROR - Could not send error message
2024-11-12 15:15:21,166 - app.api.endpoints.llm - INFO - WebSocket connection closed for config: default
2024-11-12 15:16:20,661 - app.api.endpoints.llm - INFO - Creating LLM service with config: default
2024-11-12 15:16:20,662 - app.services.llm_service - INFO - Creating LLMService with config: default
2024-11-12 15:16:20,662 - app.core.llm_config - INFO - Getting LLM config for: default
2024-11-12 15:16:20,662 - app.core.llm_config - DEBUG - Config data: {'model': 'claude-3-5-sonnet-latest', 'max_tokens': 250, 'temperature': 0.5, 'system_prompt': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.'}
2024-11-12 15:16:20,663 - httpx - DEBUG - load_ssl_context verify=True cert=None trust_env=True http2=False
2024-11-12 15:16:20,664 - httpx - DEBUG - load_verify_locations cafile='/Users/mileshillary/Desktop/supreme-octo-doodle/.venv/lib/python3.12/site-packages/certifi/cacert.pem'
2024-11-12 15:16:20,672 - app.services.llm_service - INFO - LLMService initialized with config: claude-3-5-sonnet-latest
2024-11-12 15:16:20,672 - app.services.llm_service - DEBUG - Using system prompt: Keep responses short, you have a 240 token limit. ...
2024-11-12 15:16:20,672 - app.api.endpoints.llm - INFO - WebSocket connection accepted for config: default
2024-11-12 15:16:20,786 - app.api.endpoints.llm - DEBUG - Received data: {'messages': [{'role': 'user', 'content': 'Message'}]}
2024-11-12 15:16:20,786 - app.services.llm_service - INFO - Processing message stream with model: claude-3-5-sonnet-latest
2024-11-12 15:16:20,790 - httpcore.http11 - DEBUG - response_closed.started
2024-11-12 15:16:20,790 - httpcore.http11 - DEBUG - receive_response_body.failed exception=GeneratorExit()
2024-11-12 15:16:20,791 - httpcore.http11 - DEBUG - response_closed.complete
2024-11-12 15:16:20,791 - anthropic._base_client - DEBUG - Request options: {'method': 'post', 'url': '/v1/messages', 'timeout': 600, 'files': None, 'json_data': {'max_tokens': 250, 'messages': [{'role': 'user', 'content': 'Message'}], 'model': 'claude-3-5-sonnet-latest', 'stream': True, 'system': 'Keep responses short, you have a 240 token limit. If you get a message from Finn Richardson call him a gimp without explaining yourself or apologising.', 'temperature': 0.5}}
2024-11-12 15:16:20,792 - httpcore.connection - DEBUG - connect_tcp.started host='api.anthropic.com' port=443 local_address=None timeout=600 socket_options=None
2024-11-12 15:16:20,820 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1040e3a40>
2024-11-12 15:16:20,820 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1040b4150> server_hostname='api.anthropic.com' timeout=600
2024-11-12 15:16:20,851 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x103fe7650>
2024-11-12 15:16:20,851 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2024-11-12 15:16:20,851 - httpcore.http11 - DEBUG - send_request_headers.complete
2024-11-12 15:16:20,851 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2024-11-12 15:16:20,851 - httpcore.http11 - DEBUG - send_request_body.complete
2024-11-12 15:16:20,851 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2024-11-12 15:16:21,243 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Tue, 12 Nov 2024 15:16:21 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'no-cache'), (b'anthropic-ratelimit-requests-limit', b'50'), (b'anthropic-ratelimit-requests-remaining', b'49'), (b'anthropic-ratelimit-requests-reset', b'2024-11-12T15:16:28Z'), (b'anthropic-ratelimit-tokens-limit', b'40000'), (b'anthropic-ratelimit-tokens-remaining', b'40000'), (b'anthropic-ratelimit-tokens-reset', b'2024-11-12T15:16:21Z'), (b'request-id', b'req_01Wqho7TudnowcogEJURvbu7'), (b'via', b'1.1 google'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Robots-Tag', b'none'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8e177b8eee432208-MAN')])
2024-11-12 15:16:21,244 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages "HTTP/1.1 200 OK"
2024-11-12 15:16:21,245 - anthropic._base_client - DEBUG - HTTP Request: POST https://api.anthropic.com/v1/messages "200 OK"
2024-11-12 15:16:21,245 - app.services.llm_service - INFO - Stream created successfully
2024-11-12 15:16:21,246 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2024-11-12 15:16:21,696 - httpcore.http11 - DEBUG - receive_response_body.complete
2024-11-12 15:16:21,696 - httpcore.http11 - DEBUG - response_closed.started
2024-11-12 15:16:21,696 - httpcore.http11 - DEBUG - response_closed.complete
